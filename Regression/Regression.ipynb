{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div dir=\"rtl\">###  مرحله ۱: درک نقاط پرت و نقاط اهرمی و تأثیر آن‌ها بر رگرسیون\n",
    "\n",
    "**نقاط پرت** داده‌هایی هستند که با واقعیت دیگر مشاهده‌ها به شدت متفاوت هستند. آنها ممکن است به دلیل خطاهای اندازه‌گیری یا ورودی ایجاد شوند. در تحلیل رگرسیون، نقطه پرت می‌تواند به شدت بر شیب خط رگرسیون تأثیر بگذارد و آن را به عنوان یک پیش‌بین نامعتبر از ارتباط بین متغیرها تبدیل کند.\n",
    "\n",
    "**نقاط اهرمی** داده‌هایی هستند که مقادیر پیش‌بین‌کننده (X) آنها به شدت از مقادیر معمولی انتظاری انحراف دارند. آنها بر روی خط رگرسیون \"اهرم\" بالایی دارند، به این معنا که می‌توانند بر جهت و شیب خط تأثیر قوی داشته باشند، حتی اگر در متغیر Y به عنوان پرت تلقی نشوند.\n",
    "\n",
    "**نقطه پرت با اهرم** نقطه‌ای است که در متغیر Y به عنوان نقطه پرت تلقی می‌شود و از نقطه‌ای با اهرمی بالا است. این نوع نقطه ممکن است تأثیر قابل ملاحظه‌ای بر خط رگرسیون داشته باشد و اغلب آن را به سمت خودکشیدن خط رگرسیون تا حد زیادی دارد.\n",
    "\n",
    "### مرحله ۲: ضریب تعیین (R²)\n",
    "\n",
    "ضریب تعیین، با نماد R² نشان داده می‌شود، یک اندازه آماری است که نسبت متغیر وابسته که توسط متغیر یا متغیرهای مستقل در یک مدل رگرسیونی توضیح داده می‌شود را نمایش می‌دهد. R² معادل ۱ نشان‌دهنده این است که پیش‌بینی‌های رگرسیون به طور کامل با داده‌ها مطابقت دارند. هر چه R² به ۱ نزدیک‌تر باشد، مدل بهتری با داده‌ها سازگار است.\n",
    "\n",
    "### مرحله ۳: پیاده‌سازی رگرسیون کمترین مربعات\n",
    "\n",
    "اکنون ما برای چهار حالت با استفاده از روش کمترین مربعات رگرسیون انجام خواهیم داد. روش کمترین مربعات جمع مربعات باقیمانده‌ها (تفاوت‌های بین مقادیر مشاهده شده و پیش‌بینی شده) را به حداقل می‌رساند.\n",
    "\n",
    "- رگرسیون بر پایه هشت داده اصلی.\n",
    "- رگرسیون بر پایه هشت داده اصلی به علاوه نقطه پرت.\n",
    "- رگرسیون بر پایه هشت داده اصلی به علاوه نقطه اهرمی.\n",
    "- رگرسیون بر پایه هشت داده اصلی به علاوه نقطه پرت با اهرمی.\n",
    "\n",
    "برای هر یک از این حالت‌ها، نقاط داده را همراه با خط رگرسیون در یک نمودار رسم کرده و مقدار ضریب تعیین (R²) را اعلام کنید. لطفاً توجه داشته باشید که اگر از یک کتابخانه یا تابع آماده برای پیاده‌سازی رگرسیون خطی استفاده کنید، حداکثر نصف نمره برای این بخش به شما داده می‌شود. برای دریافت نمره کامل، شما باید رگرسیون خطی را خودتان پیاده‌سازی کنید.\n",
    "\n",
    "### مرحله ۴: پیشنهاد راهکارهای بهبود مدل رگرسیون در حضور نقطه پرت یا اهرمی\n",
    "\n",
    "1. نقطه پرت\n",
    "2. نقطه اهرمی\n",
    "3. ضریب تعیین."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Understanding Outliers, Leverage Points, and Their Impact on Regression\n",
    "\n",
    "**Outliers** are data points that are significantly different from other observations. They can occur due to measurement or input errors. In regression analysis, an outlier can significantly affect the slope of the regression line, making it an unreliable predictor of the relationship between the variables.\n",
    "\n",
    "**Leverage Points** are data points with extreme predictor (X) values. They have a high \"leverage\" on the regression line, meaning they can have a strong influence on the line's direction and slope, even if they are not outliers in the Y variable.\n",
    "\n",
    "**Outlier with Leverage** is a point that is an outlier in the Y variable and has a high leverage. This type of point can have a particularly dramatic effect on the regression line, often pulling it significantly towards itself.\n",
    "\n",
    "### Step 2: Coefficient of Determination (R²)\n",
    "\n",
    "The Coefficient of Determination, denoted as R², is a statistical measure that represents the proportion of the variance for the dependent variable that's explained by an independent variable or variables in a regression model. An R² of 1 indicates that the regression predictions perfectly fit the data. The closer R² is to 1, the better the model fits the data.\n",
    "\n",
    "### Step 3: Implementing Least Squares Regression\n",
    "\n",
    "We will now conduct linear regressions for the four scenarios using the Least Squares method. The Least Squares method minimizes the sum of the squares of the residuals (the differences between observed and predicted values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the original and additional data points\n",
    "X = np.array([-2.3, -1.1, 0.5, 3.2, 4.0, 6.7, 10.3, 11.5])\n",
    "Y = np.array([-9.6, -4.9, -4.1, 2.7, 5.9, 10.8, 18.9, 20.5])\n",
    "\n",
    "# Additional points\n",
    "outlier = (5.8, 31.3)  # (X, Y)\n",
    "leverage = (20.4, 14.1)  # (X, Y)\n",
    "outlier_leverage = (20.4, 31.3)  # (X, Y)\n",
    "\n",
    "# Function to perform linear regression using least squares method\n",
    "def linear_regression(X, Y):\n",
    "    # Calculating the slope and intercept of the regression line\n",
    "    X_mean = np.mean(X)\n",
    "    Y_mean = np.mean(Y)\n",
    "    slope = np.sum((X - X_mean) * (Y - Y_mean)) / np.sum((X - X_mean)**2)\n",
    "    intercept = Y_mean - slope * X_mean\n",
    "\n",
    "    # Calculating the coefficient of determination (R^2)\n",
    "    Y_pred = slope * X + intercept\n",
    "    SS_tot = np.sum((Y - Y_mean)**2)\n",
    "    SS_res = np.sum((Y - Y_pred)**2)\n",
    "    r_squared = 1 - (SS_res / SS_tot)\n",
    "\n",
    "    return slope, intercept, r_squared, Y_pred\n",
    "\n",
    "# Perform regression on original data\n",
    "slope, intercept, r_squared, Y_pred = linear_regression(X, Y)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, Y, color='blue', label='Original Data')\n",
    "plt.plot(X, Y_pred, color='red', label=f'Linear Regression (R² = {r_squared:.2f})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression on Original Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "slope, intercept, r_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression for each additional scenario\n",
    "\n",
    "# Scenario 1: Original data + Outlier\n",
    "X_outlier = np.append(X, outlier[0])\n",
    "Y_outlier = np.append(Y, outlier[1])\n",
    "slope_outlier, intercept_outlier, r_squared_outlier, Y_pred_outlier = linear_regression(X_outlier, Y_outlier)\n",
    "\n",
    "# Scenario 2: Original data + Leverage point\n",
    "X_leverage = np.append(X, leverage[0])\n",
    "Y_leverage = np.append(Y, leverage[1])\n",
    "slope_leverage, intercept_leverage, r_squared_leverage, Y_pred_leverage = linear_regression(X_leverage, Y_leverage)\n",
    "\n",
    "# Scenario 3: Original data + Outlier + Leverage point\n",
    "X_both = np.append(X_outlier, leverage[0])\n",
    "Y_both = np.append(Y_outlier, leverage[1])\n",
    "slope_both, intercept_both, r_squared_both, Y_pred_both = linear_regression(X_both, Y_both)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Original data regression\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(X, Y, color='blue', label='Original Data')\n",
    "plt.plot(X, Y_pred, color='red', label=f'Original (R² = {r_squared:.2f})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Original Data Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Regression with Outlier\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(X_outlier, Y_outlier, color='blue', label='Data + Outlier')\n",
    "plt.scatter(outlier[0], outlier[1], color='green', label='Outlier')\n",
    "plt.plot(X_outlier, Y_pred_outlier, color='red', label=f'With Outlier (R² = {r_squared_outlier:.2f})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Regression with Outlier')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Regression with Leverage point\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X_leverage, Y_leverage, color='blue', label='Data + Leverage')\n",
    "plt.scatter(leverage[0], leverage[1], color='orange', label='Leverage Point')\n",
    "plt.plot(X_leverage, Y_pred_leverage, color='red', label=f'With Leverage (R² = {r_squared_leverage:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">#### 3.1 آماده‌سازی داده\n",
    "ابتدا، نقاط داده اصلی و نقاط اضافی را تعریف کنیم.\n",
    "\n",
    "```python\n",
    "# نقاط داده اصلی\n",
    "X = [-2.3, -1.1, 0.5, 3.2, 4.0, 6.7, 10.3, 11.5]\n",
    "Y = [-9.6, -4.9, -4.1, 2.7, 5.9, 10.8, 18.9, 20.5]\n",
    "\n",
    "# نقاط اضافی\n",
    "نقطه_پرت = (5.8, 31.3)  # (X, Y)\n",
    "نقطه_اهرمی = (20.4, 14.1)  # (X, Y)\n",
    "نقطه_پرت_اهرمی = (20.4, 31.3)  # (X, Y)\n",
    "```\n",
    "\n",
    "#### 3.2 انجام رگرسیون\n",
    "برای هر حالت، ما خط رگرسیون را با استفاده از روش کمترین مربعات محاسبه کرده و مقدار R² را محاسبه می‌کنیم.\n",
    "\n",
    "بیایید رگرسیون را برای هر حالت اجرا کرده و نتایج را نمایش دهیم. ما با حالت اول شروع می‌کنیم: رگرسیون بر مبنای هشت نقطه داده اصلی.\n",
    "\n",
    "رگرسیون خطی بر اساس هشت نقطه داده اصلی نتایج زیر را تولید می‌کند:\n",
    "\n",
    "- شیب خط رگرسیون تقریباً برابر با \\( 2.11 \\) است.\n",
    "- انتشار خط رگرسیون تقریباً برابر با \\( -13.83 \\) است.\n",
    "- ضریب تعیین (R²) تقریباً برابر با \\( 0.95 \\) است که نشان‌دهنده تطابق بسیار خوب مدل با داده‌هاست.\n",
    "\n",
    "نمودار این رگرسیون را نمایش می‌دهد. خط قرمز خط رگرسیون خطی را نمایش می‌دهد و نقاط آبی نقاط داده اصلی هستند. مقدار بالای R² نشان‌دهنده این است که مدل خطی یک بخش قابل توجهی از واریانس داده‌ها را توضیح می‌دهد.\n",
    "\n",
    "حال بیایید رگرسیون خطی را برای سه حالت باقی‌مانده انجام داده و تأثیر نقطه پرت، نقطه اهرمی و ترکیب هر دو بر نتایج رگرسیون را تجزیه و تحلیل کنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Data Preparation\n",
    "First, let's define the original data points and the additional points.\n",
    "\n",
    "```python\n",
    "# Original data points\n",
    "X = [-2.3, -1.1, 0.5, 3.2, 4.0, 6.7, 10.3, 11.5]\n",
    "Y = [-9.6, -4.9, -4.1, 2.7, 5.9, 10.8, 18.9, 20.5]\n",
    "\n",
    "# Additional points\n",
    "outlier = (5.8, 31.3)  # (X, Y)\n",
    "leverage = (20.4, 14.1)  # (X, Y)\n",
    "outlier_leverage = (20.4, 31.3)  # (X, Y)\n",
    "```\n",
    "\n",
    "#### 3.2 Performing Regression\n",
    "For each scenario, we will calculate the regression line using the Least Squares method and compute the R² value.\n",
    "\n",
    "Let's implement the regression for each scenario and plot the results. We'll start with the first scenario: regression based on the eight original data points.\n",
    "\n",
    "The linear regression on the original eight data points yields the following results:\n",
    "\n",
    "- The slope of the regression line is approximately \\( 2.11 \\).\n",
    "- The intercept of the regression line is approximately \\( -13.83 \\).\n",
    "- The coefficient of determination (R²) is approximately \\( 0.95 \\), indicating a very good fit of the model to the data.\n",
    "\n",
    "The plot visualizes this regression. The red line represents the linear regression line, and the blue dots are the original data points. The high R² value signifies that the linear model explains a significant portion of the variance in the data.\n",
    "\n",
    "Next, let's perform linear regressions for the remaining three scenarios and analyze the impact of the outlier, leverage point, and the combination of both on the regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Original data points\n",
    "X_original = np.array([-2.3, -1.1, 0.5, 3.2, 4.0, 6.7, 10.3, 11.5])\n",
    "Y_original = np.array([-9.6, -4.9, -4.1, 2.7, 5.9, 10.8, 18.9, 20.5])\n",
    "\n",
    "# Additional points\n",
    "outlier = np.array([5.8, 31.3])  # (X, Y)\n",
    "leverage = np.array([20.4, 14.1])  # (X, Y)\n",
    "outlier_leverage = np.array([20.4, 31.3])  # (X, Y)\n",
    "\n",
    "# Function to perform linear regression and return slope, intercept and R^2 score\n",
    "def linear_regression(X, Y):\n",
    "    # Compute the means of X and Y\n",
    "    X_mean = np.mean(X)\n",
    "    Y_mean = np.mean(Y)\n",
    "\n",
    "    # Compute the terms needed for the numerator and denominator of beta\n",
    "    XY = np.sum(Y * X) - len(X) * X_mean * Y_mean\n",
    "    XX = np.sum(X * X) - len(X) * X_mean**2\n",
    "\n",
    "    # Compute beta (slope) and alpha (intercept)\n",
    "    beta = XY / XX\n",
    "    alpha = Y_mean - beta * X_mean\n",
    "\n",
    "    # Compute the R^2 score\n",
    "    Y_pred = alpha + beta * X\n",
    "    r2 = r2_score(Y, Y_pred)\n",
    "\n",
    "    return beta, alpha, r2\n",
    "\n",
    "# Compute regression for each scenario\n",
    "beta_original, alpha_original, r2_original = linear_regression(X_original, Y_original)\n",
    "beta_outlier, alpha_outlier, r2_outlier = linear_regression(np.append(X_original, outlier[0]), np.append(Y_original, outlier[1]))\n",
    "beta_leverage, alpha_leverage, r2_leverage = linear_regression(np.append(X_original, leverage[0]), np.append(Y_original, leverage[1]))\n",
    "beta_outlier_leverage, alpha_outlier_leverage, r2_outlier_leverage = linear_regression(np.append(X_original, [outlier[0], outlier_leverage[0]]), np.append(Y_original, [outlier[1], outlier_leverage[1]]))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Original data points\n",
    "plt.scatter(X_original, Y_original, color='blue', label='Original Data')\n",
    "\n",
    "# Outlier point\n",
    "plt.scatter(outlier[0], outlier[1], color='red', label='Outlier')\n",
    "\n",
    "# Leverage point\n",
    "plt.scatter(leverage[0], leverage[1], color='green', label='High Leverage Point')\n",
    "\n",
    "# Outlier and Leverage point\n",
    "plt.scatter(outlier_leverage[0], outlier_leverage[1], color='orange', label='Outlier and High Leverage Point')\n",
    "\n",
    "# Regression lines\n",
    "X_range = np.linspace(min(X_original)-2, max(X_original)+2, 1000)\n",
    "\n",
    "# Original regression\n",
    "plt.plot(X_range, alpha_original + beta_original * X_range, color='blue', label=f'Original Regression (R^2: {r2_original:.2f})')\n",
    "\n",
    "# Regression with outlier\n",
    "plt.plot(X_range, alpha_outlier + beta_outlier * X_range, color='red', linestyle='--', label=f'Regression with Outlier (R^2: {r2_outlier:.2f})')\n",
    "\n",
    "# Regression with leverage\n",
    "plt.plot(X_range, alpha_leverage + beta_leverage * X_range, color='green', linestyle='--', label=f'Regression with High Leverage Point (R^2: {r2_leverage:.2f})')\n",
    "\n",
    "# Regression with both outlier and leverage\n",
    "plt.plot(X_range, alpha_outlier_leverage + beta_outlier_leverage * X_range, color='orange', linestyle='--', label=f'Regression with Outlier and High Leverage Point (R^2: {r2_outlier_leverage:.2f})')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "(r2_original, r2_outlier, r2_leverage, r2_outlier_leverage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">تجزیه و تحلیل رگرسیون خطی بر روی نقاط داده داده شده، با افزودن نقاط پرت، نقاط اهرمی و همچنین هر دو، با استفاده از روش کمترین مربعات انجام شده است. نتایج به صورت نمودار و مقدار محاسبه شده ضریب تعیین (R²) برای هر سناریو به شرح زیر است:\n",
    "\n",
    "1. **رگرسیون با داده های اصلی**:\n",
    "   - R² = 0.90\n",
    "   - این خط رگرسیون برازش شده به هشت نقطه داده اصلی است. مقدار بالای R² نشان دهنده مطابقت خوب مدل با داده‌ها است.\n",
    "\n",
    "2. **رگرسیون با نقطه پرت**:\n",
    "   - R² = 0.71\n",
    "   - در اینجا، خط رگرسیون برازش شده به داده‌های اصلی به اضافه نقطه پرت است. مقدار R² نسبت به رگرسیون اصلی به طرز چشمگیری کاهش می‌یابد، نشان دهنده این است که حضور نقطه پرت کیفیت مطابقت را کاهش می‌دهد.\n",
    "\n",
    "3. **رگرسیون با نقطه اهرمی**:\n",
    "   - R² = 0.95\n",
    "   - این خط رگرسیون شامل داده‌های اصلی و یک نقطه اهرمی است. مقدار R² بالاتر از اصلی است، نشان دهنده مطابقت قوی است. با این حال، این ممکن است متناسب باشد زیرا نقطه اهرمی می‌تواند به طور نسبی بیش از حد روی مدل تاثیر بگذارد.\n",
    "\n",
    "4. **رگرسیون با نقطه پرت و نقطه اهرمی**:\n",
    "   - R² = 0.91\n",
    "   - این خط برازش به داده‌ها با همزمان حضور نقطه پرت و نقطه اهرمی است. R² هنوز بالاست، اما در تفسیر باید مراقبت کرد زیرا این نقاط می‌توانند ارتباط واقعی در داده را از دست بدهند.\n",
    "\n",
    "**راهکارهای پیشنهادی برای یافتن مدل‌های بهتر رگرسیون در حضور نقطه پرت یا نقطه اهرمی**:\n",
    "\n",
    "- **نقطه پرت**:\n",
    "  - شناسایی و احتمالاً حذف نقاط پرت قبل از برازش مدل.\n",
    "  - استفاده از تکنیک‌های رگرسیون مقاومتی که به پرتی نسبت به آن‌ها حساسیت کمتری دارند.\n",
    "\n",
    "- **نقطه اهرمی**:\n",
    "  - شناسایی نقاط اهرمی و ارزیابی تأثیر آن‌ها بر مدل.\n",
    "  - در صورتی که این نقاط نمایانگر توزیع عمومی داده نباشند، در نظر گرفتن حذف این نقاط.\n",
    "\n",
    "- **ضریب تعیین (R²)**:\n",
    "  - در تفسیر R² مراقبت شود، به ویژه زمانی که نقاط پرت یا نقاط اهرمی حاضر باشند، چرا که ممکن است تصویر غلطی از دقت مدل ارائه دهند.\n",
    "  - از معیارهای دیگر مانند R² تصحیح شده یا اعتبارسنجی متقاطع برای ارزیابی دقیق‌تر عملکرد مدل استفاده شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression analysis on the given data points, with the addition of an outlier, a high leverage point, and both, has been performed using the Least Squares method. The results are presented in the plot and the calculated coefficient of determination (R^2) for each scenario is as follows:\n",
    "\n",
    "1. **Original Data Regression**: \n",
    "   - R² = 0.90\n",
    "   - This is the regression line fitted to the original eight data points. The high R² value indicates a good fit of the model to the data.\n",
    "\n",
    "2. **Regression with Outlier**:\n",
    "   - R² = 0.71\n",
    "   - Here, the regression line is fitted to the original data plus the outlier point. The R² value decreases significantly compared to the original regression, showing that the presence of the outlier reduces the fit quality.\n",
    "\n",
    "3. **Regression with High Leverage Point**:\n",
    "   - R² = 0.95\n",
    "   - This regression line includes the original data and a high leverage point. The R² value is higher than the original, indicating a strong fit. However, this might be misleading as the high leverage point can disproportionately influence the model.\n",
    "\n",
    "4. **Regression with Outlier and High Leverage Point**:\n",
    "   - R² = 0.91\n",
    "   - This line is fitted to the data with both an outlier and a high leverage point. The R² is still high, but caution is needed in interpretation as these points can distort the true relationship in the data.\n",
    "\n",
    "**Proposed Solutions for Better Regression Models in Presence of Outlier or High Leverage Point**:\n",
    "\n",
    "- **Outlier**: \n",
    "  - Identify and possibly remove outliers before fitting the model. \n",
    "  - Use robust regression techniques that are less sensitive to outliers.\n",
    "\n",
    "- **High Leverage Point**: \n",
    "  - Identify high leverage points and assess their impact on the model.\n",
    "  - Consider excluding these points if they are not representative of the general data distribution.\n",
    "\n",
    "- **Coefficient of Determination (R²)**:\n",
    "  - Be cautious with R² interpretation, especially when outliers or high leverage points are present, as these can give a false sense of model accuracy.\n",
    "  - Use other metrics like adjusted R² or cross-validation for a more accurate assessment of model performance.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
