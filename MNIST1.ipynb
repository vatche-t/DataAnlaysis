{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Task:\n",
    "+ We are tasked with analyzing a pretrained autoencoder neural network model, specifically focusing on its Mean Squared Error (MSE) performance on the MNIST dataset. The autoencoder is stored in an .h5 file named mnist_AE.+ \n",
    "h5. Our aim is to test whether the MSE data follows a normal distribution using the Kolmogorov-Smirnov test.\n",
    "\n",
    "## Setting Up the Environment: \n",
    "+ We start by setting up the Python environment and importing necessary libraries. The set_seed function ensures reproducibility in our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vatche\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from scipy import stats\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(810109203)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pretrained Model: We load the mnist_AE.h5 file, which contains the pretrained autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vatche\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('mnist_AE.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the MNIST Dataset: \n",
    "\n",
    "+ The MNIST dataset is a collection of handwritten digit images, commonly used for training and testing in the field of machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "+  We use the encoder and decoder parts of the autoencoder to compress and reconstruct the MNIST images. We then calculate the MSE between the original and reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test data to match the input shape expected by the model\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the reconstructed images to match the original images' shape\n",
    "reconstructed_reshaped = reconstructed.reshape(x_test.shape)\n",
    "\n",
    "# Calculate MSE for each image\n",
    "mse = np.mean(np.square(x_test - reconstructed_reshaped), axis=(1, 2, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Test:\n",
    "+ Finally, we perform the Kolmogorov-Smirnov test to check if the MSE values follow a normal distribution, using the mean and standard deviation of these MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Statistic: 0.0700142082808849, P-value: 4.5397725566600726e-43\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of MSE\n",
    "mean = np.mean(mse)\n",
    "std = np.std(mse)\n",
    "\n",
    "# Perform the Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = stats.kstest(mse, cdf='norm', args=(mean, std))\n",
    "\n",
    "print(f\"KS Statistic: {ks_statistic}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Preparation\n",
    "### We'll import TensorFlow, NumPy, and other required libraries. We'll also set the seed as per the provided instructions for consistency in random values. Then, we'll load the MNIST dataset and preprocess it for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 6s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from scipy import stats\n",
    "\n",
    "# Set the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(810109203)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, 28*28))\n",
    "x_test = x_test.reshape((-1, 28*28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building and Training the Autoencoder\n",
    "#### We'll create a simple autoencoder model. The encoder will compress the MNIST images into a lower-dimensional latent space, and the decoder will try to reconstruct the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vatche\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Vatche\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\Vatche\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "235/235 [==============================] - 4s 10ms/step - loss: 0.2213 - val_loss: 0.1422\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.1256 - val_loss: 0.1116\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.1065 - val_loss: 0.1021\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0994 - val_loss: 0.0957\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0953 - val_loss: 0.0925\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0920 - val_loss: 0.0893\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0893 - val_loss: 0.0879\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0871 - val_loss: 0.0850\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0853 - val_loss: 0.0840\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0839 - val_loss: 0.0827\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0828 - val_loss: 0.0815\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0820 - val_loss: 0.0805\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0811 - val_loss: 0.0800\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0805 - val_loss: 0.0793\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0799 - val_loss: 0.0795\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0794 - val_loss: 0.0784\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0789 - val_loss: 0.0780\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0785 - val_loss: 0.0775\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0781 - val_loss: 0.0772\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0777 - val_loss: 0.0770\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0773 - val_loss: 0.0766\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0770 - val_loss: 0.0763\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0767 - val_loss: 0.0759\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0762 - val_loss: 0.0754\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0760 - val_loss: 0.0756\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0758 - val_loss: 0.0752\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0756 - val_loss: 0.0749\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0754 - val_loss: 0.0752\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0752 - val_loss: 0.0748\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0751 - val_loss: 0.0745\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0749 - val_loss: 0.0748\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0748 - val_loss: 0.0743\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0747 - val_loss: 0.0741\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0745 - val_loss: 0.0740\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0744 - val_loss: 0.0738\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0743 - val_loss: 0.0738\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0742 - val_loss: 0.0735\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0740 - val_loss: 0.0734\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0739 - val_loss: 0.0735\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0738 - val_loss: 0.0733\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0737 - val_loss: 0.0735\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0736 - val_loss: 0.0732\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0735 - val_loss: 0.0730\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0735 - val_loss: 0.0730\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0734 - val_loss: 0.0728\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0733 - val_loss: 0.0728\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0732 - val_loss: 0.0729\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0732 - val_loss: 0.0727\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0731 - val_loss: 0.0727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x264107b4a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autoencoder model\n",
    "input_img = Input(shape=(28*28,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(28*28, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Reconstruction and MSE Calculation\n",
    "### We'll use the trained autoencoder to reconstruct the test set and calculate the MSE for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict (reconstruct) the test set\n",
    "reconstructed_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = np.mean(np.power(x_test - reconstructed_imgs, 2), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Normality Test\n",
    "### Finally, we'll conduct a Kolmogorov-Smirnov test to check the normality of the MSE distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Statistic: 0.09343222023897935, P-value: 2.0121003016299243e-76\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(mse)\n",
    "std = np.std(mse)\n",
    "\n",
    "# KS test for normality\n",
    "ks_statistic, p_value = stats.kstest(mse, cdf='norm', args=(mean, std))\n",
    "print(f'KS Statistic: {ks_statistic}, P-value: {p_value}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
