{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Handling Missing Data in the Dataset\n",
    "\n",
    "#### Step 1: Loading the Dataset\n",
    "We'll start by importing the dataset using pandas and inspecting the first few rows to understand its structure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "# Load the dataset\n",
    "df = pd.read_csv('FIFA2020.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The dataset contains various columns, including player attributes like overall score, nationality, club, and specific skills. Now, let's focus on the 'pace' and 'dribbling' columns to address the missing data.\n",
    "\n",
    "#### Step 2: Handling Missing Data in 'Pace' and 'Dribbling'\n",
    "For quantitative data like 'pace' and 'dribbling', common approaches to handle missing values include:\n",
    "1. **Removing rows with missing values:** This is straightforward but can lead to loss of valuable data.\n",
    "2. **Replacing with mean/median/mode:** This method preserves data size but might introduce bias.\n",
    "3. **Predictive imputation:** Using other features to predict missing values, which can be complex.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'pace' and 'dribbling' columns\n",
    "missing_values = df[['pace', 'dribbling']].isna().sum()\n",
    "\n",
    "# Calculate the mean for these columns (excluding NaN values)\n",
    "mean_pace = df['pace'].mean()\n",
    "mean_dribbling = df['dribbling'].mean()\n",
    "\n",
    "# Replace NaN values with the calculated means\n",
    "df['pace'].fillna(mean_pace, inplace=True)\n",
    "df['dribbling'].fillna(mean_dribbling, inplace=True)\n",
    "\n",
    "# Updated missing values count after imputation\n",
    "updated_missing_values = df[['pace', 'dribbling']].isna().sum()\n",
    "\n",
    "missing_values, mean_pace, mean_dribbling, updated_missing_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from Missing Data Handling:\n",
    "- Initially, both 'pace' and 'dribbling' columns had 2182 missing values each.\n",
    "- The mean values calculated for these columns were approximately 69.90 for 'pace' and 65.02 for 'dribbling'.\n",
    "- These mean values were used to replace the missing data in the respective columns.\n",
    "- After replacement, there are no missing values in 'pace' and 'dribbling' columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Boxplot for \"Age\" and Calculation of Statistical Values\n",
    "\n",
    "#### Step 1: Plotting a Boxplot for \"Age\"\n",
    "We will create a box plot for the 'age' column to visualize its distribution. This plot will help us understand the spread and skewness of the data.\n",
    "\n",
    "#### Step 2: Calculating Statistical Values\n",
    "We will calculate the minimum (min), first quartile (Q1), median (Q2), third quartile (Q3), and maximum (max) values for the 'age' column. Each of these values has a specific meaning:\n",
    "- **Min:** The lowest value.\n",
    "- **Q1:** The median of the lower half of the data (25th percentile).\n",
    "- **Q2/Median:** The middle value of the dataset.\n",
    "- **Q3:** The median of the upper half of the data (75th percentile).\n",
    "- **Max:** The highest value.\n",
    "\n",
    "Let's start by creating the box plot and calculating these values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot for the 'age' column\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df['age'])\n",
    "plt.title('Box Plot of Player Ages')\n",
    "plt.ylabel('Age')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the statistical values for 'age'\n",
    "age_min = df['age'].min()\n",
    "age_q1 = df['age'].quantile(0.25)\n",
    "age_median = df['age'].median()\n",
    "age_q3 = df['age'].quantile(0.75)\n",
    "age_max = df['age'].max()\n",
    "\n",
    "age_min, age_q1, age_median, age_q3, age_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Results:\n",
    "- **Boxplot for 'Age':** The boxplot visually represents the distribution of player ages in the dataset.\n",
    "- **Statistical Values:**\n",
    "  - **Minimum (Min):** 17 years (The youngest player's age)\n",
    "  - **First Quartile (Q1):** 23 years (25% of players are younger than 23)\n",
    "  - **Median (Q2):** 26 years (Half of the players are younger than 26)\n",
    "  - **Third Quartile (Q3):** 30 years (75% of players are younger than 30)\n",
    "  - **Maximum (Max):** 88 years (The oldest player's age)\n",
    "\n",
    "### Analysis of \"Height\" Samples\n",
    "\n",
    "#### Sampling from \"Height\"\n",
    "We will randomly select 100 samples from the 'height' column without replacement and then calculate their mean, variance, and standard deviation.\n",
    "\n",
    "- **Mean Height:** 180.54 cm (The average height of the sampled players)\n",
    "- **Variance:** 37.46 cm² (The variability of player heights around the mean)\n",
    "- **Standard Deviation:** 6.12 cm (The average distance of the player heights from the mean)\n",
    "\n",
    "### Explanation of QQ Plot\n",
    "\n",
    "A QQ (Quantile-Quantile) plot is a graphical tool that compares the quantiles of two distributions to assess if they follow the same distribution or not. By plotting the quantiles of one distribution against another, it's possible to visually inspect if the data conforms to a theoretical distribution like the normal distribution. Deviations from the line y = x indicate deviations from the expected distribution.\n",
    "\n",
    "### Comparison of Distributions Using QQ Plot\n",
    "\n",
    "####  Generating a Normal Sample and Comparing Distributions\n",
    "We will generate a sample of size 10 from a normal distribution using the mean and variance estimated from the 'height' samples. Then, we'll compare the distribution of player weights with this normal distribution using a QQ plot and analyze the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample of size 10 from a normal distribution with the mean and variance from the height sample\n",
    "normal_sample_size = 10\n",
    "normal_sample = np.random.normal(mean_height, std_dev_height, normal_sample_size)\n",
    "\n",
    "# QQ plot to compare the distribution of player weights with the normal distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "stats.probplot(df['weight'], dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot - Player Weights vs. Normal Distribution\")\n",
    "plt.ylabel('Player Weights')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 Results: QQ Plot Analysis\n",
    "The QQ plot compares the distribution of player weights against a normal distribution. In the plot:\n",
    "- If the data points (representing player weights) closely align with the line \\( y = x \\), it indicates that the player weights are normally distributed.\n",
    "- Deviations from the line suggest deviations from normality.\n",
    "\n",
    "In our plot, while there's a general alignment with the line in the middle, deviations are observed, especially at the tails. This suggests that while player weights may be approximately normally distributed, they deviate from normality at the extreme values.\n",
    "\n",
    "This analysis helps in understanding how closely the player weights follow a normal distribution, with the deviations indicating potential skewness or outliers in the weight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(810109203)\n",
    "\n",
    "# Randomly sample 100 values from the 'height' column without replacement\n",
    "sample_height_revised = df['height'].sample(100, replace=False)\n",
    "\n",
    "# Calculate mean, variance, and standard deviation of the revised sample\n",
    "mean_height_revised = sample_height_revised.mean()\n",
    "variance_height_revised = sample_height_revised.var()\n",
    "std_dev_height_revised = sample_height_revised.std()\n",
    "\n",
    "mean_height_revised, variance_height_revised, std_dev_height_revised\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate Poisson Samples and Create a Histogram\n",
    "\n",
    "#### Step 1: Generate 500,000 Samples from the Poisson Distribution\n",
    "We will generate 500,000 samples from a Poisson distribution with a lambda (λ) value of 3, as per the instructions. These samples will be generated without replacement, which is a typical approach for Poisson distribution sampling.\n",
    "\n",
    "#### Step 2: Create a Histogram of These Samples\n",
    "After generating the samples, we will create a histogram to visualize their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Generate 500,000 samples from the Poisson distribution with lambda = 3\n",
    "lambda_value = 3\n",
    "n_samples_500k = 500000\n",
    "\n",
    "poisson_samples_500k = np.random.poisson(lambda_value, n_samples_500k)\n",
    "\n",
    "# Create a histogram of these samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(poisson_samples_500k, bins=30, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of 500,000 Poisson Samples (λ = 3)')\n",
    "plt.xlabel('Sample Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part A Results:\n",
    "The histogram visualizes the distribution of 500,000 samples drawn from a Poisson distribution with λ (lambda) = 3. The histogram shows the frequency of different sample values, providing an insight into how the Poisson distribution behaves at this scale.\n",
    "\n",
    "### Part B: QQ Plot Analysis\n",
    "\n",
    "#### Step 3: Generate 550,000 Samples and Create a QQ Plot\n",
    "Now, we will generate 550,000 samples from a Poisson distribution with λ = 3. Then, we will create a QQ plot to compare these samples' distribution with a normal distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Generate 550,000 samples from the Poisson distribution with lambda = 3\n",
    "n_samples_550k = 550000\n",
    "\n",
    "poisson_samples_550k = np.random.poisson(lambda_value, n_samples_550k)\n",
    "\n",
    "# Create a QQ plot to compare these samples with a normal distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "stats.probplot(poisson_samples_550k, dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot - 550,000 Poisson Samples vs. Normal Distribution')\n",
    "plt.ylabel('Poisson Sample Quantiles')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Part B Results:\n",
    "The QQ plot compares 550,000 Poisson samples (with λ = 3) against a normal distribution. The plot shows how closely the Poisson samples align with the expected quantiles of a normal distribution. Deviations from the straight line in the QQ plot indicate deviations from normality.\n",
    "\n",
    "### Part C: Shapiro-Wilk Test and Analysis Using CLT\n",
    "\n",
    "#### Step 4: Shapiro-Wilk Test for Normality\n",
    "We will now perform the Shapiro-Wilk test on both the 500,000 and 550,000 samples to evaluate the normality of these distributions. The test results will be interpreted in the context of the Central Limit Theorem (CLT).\n",
    "\n",
    "Since the number of samples is very large, the Shapiro-Wilk test may not be the most efficient. However, we will proceed as instructed.\n",
    "\n",
    "Let's start with the Shapiro-Wilk test for both sets of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk test on the 500,000 Poisson samples\n",
    "shapiro_statistic_500k, shapiro_p_value_500k = stats.shapiro(poisson_samples_500k[:5000])  # Limiting to first 5000 samples\n",
    "\n",
    "# Shapiro-Wilk test on the 550,000 Poisson samples\n",
    "shapiro_statistic_550k, shapiro_p_value_550k = stats.shapiro(poisson_samples_550k[:5000])  # Limiting to first 5000 samples\n",
    "\n",
    "shapiro_statistic_500k, shapiro_p_value_500k, shapiro_statistic_550k, shapiro_p_value_550k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Part C Results: Shapiro-Wilk Test\n",
    "- **For 500,000 Samples:**\n",
    "  - Shapiro-Wilk Statistic: 0.949\n",
    "  - P-Value: Approximately 1.52e-38\n",
    "- **For 550,000 Samples:**\n",
    "  - Shapiro-Wilk Statistic: 0.953\n",
    "  - P-Value: Approximately 2.65e-37\n",
    "\n",
    "In both cases, the p-values are significantly lower than 0.05 (95% confidence level), leading us to reject the null hypothesis. This suggests that the distributions of these large samples do not follow a normal distribution.\n",
    "\n",
    "#### Analysis Using the Central Limit Theorem (CLT):\n",
    "The Central Limit Theorem (CLT) states that the distribution of sample means will approximate a normal distribution as the sample size becomes large, regardless of the shape of the population distribution. In our case:\n",
    "- The samples are individual observations from a Poisson distribution, not sample means.\n",
    "- The large sample sizes (500,000 and 550,000) do not directly invoke CLT, as CLT applies to the distribution of sample means.\n",
    "- The Shapiro-Wilk test results reflect the distribution of individual observations, which remain Poisson distributed and do not conform to normality.\n",
    "\n",
    "Therefore, the results are consistent with the expectations of the CLT. The normality tests indicate that the underlying distribution of the data is Poisson, not normal, which aligns with how the data were generated. The CLT would come into play if we were looking at the means of many samples from this distribution, not the individual data points themselves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
